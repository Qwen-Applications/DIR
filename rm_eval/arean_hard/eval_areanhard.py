import argparse
import asyncio
import json
import os
import aiohttp
from tqdm.asyncio import tqdm_asyncio
from transformers import AutoTokenizer
from typing import Dict, Any

# è¿™ä¸ªå‡½æ•°æœ¬èº«ä¸éœ€è¦æ”¹å˜ï¼Œå› ä¸ºå®ƒå·²ç»æ¥æ”¶äº† max_tokens å‚æ•°
# å…³é”®åœ¨äºè°ƒç”¨å®ƒçš„æ—¶å€™ä¼ å…¥æ­£ç¡®çš„å€¼
async def generate_response(
    session: aiohttp.ClientSession,
    api_url: str,
    api_key: str,
    model_name: str,
    prompt_data: Dict[str, Any],
    temperature: float,
    max_tokens: int, # è¿™ä¸ªå€¼ç°åœ¨å°†æ˜¯åŠ¨æ€è®¡ç®—çš„
) -> Dict[str, Any]:
    """
    å‘ vLLM çš„ Chat API å‘é€å•ä¸ªè¯·æ±‚å¹¶è·å–å“åº”ã€‚
    """
    headers = {"Authorization": f"Bearer {api_key}"}
    
    payload = {
        "model": model_name,
        "messages": [
            {"role": "user", "content": prompt_data["prompt"]}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens, # ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„å€¼
        "stop": ["<|eot_id|>", "<|end_of_text|>"]
    }
    
    chat_completions_url = os.path.join(api_url, "chat/completions")

    try:
        async with session.post(chat_completions_url, headers=headers, json=payload) as response:
            response.raise_for_status()
            result = await response.json()
            generated_text = result["choices"][0]["message"]["content"].strip()
            
            output_record = prompt_data.copy()
            output_record["response"] = generated_text
            output_record["generator"] = model_name
            
            return output_record
            
    except aiohttp.ClientError as e:
        print(f"API è¯·æ±‚å¤±è´¥: {e} | Prompt UID: {prompt_data.get('uid', 'N/A')}")
        output_record = prompt_data.copy()
        output_record["response"] = f"ERROR: {e}"
        output_record["generator"] = model_name
        return output_record


async def main():
    """ä¸»æ‰§è¡Œå‡½æ•°ï¼ŒåŒ…å«é¢„è¿‡æ»¤å’Œå¹¶å‘è¯„ä¼°ã€‚"""
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ vLLM æœåŠ¡å¹¶å‘è¯„ä¼° LoRA æ¨¡å‹ï¼Œå¹¶è¿‡æ»¤è¶…é•¿è¾“å…¥ã€‚")
    # --- æ–°å¢å‚æ•° ---
    parser.add_argument("--tokenizer-path", required=True, help="ç”¨äºè®¡ç®— prompt é•¿åº¦çš„åŸºç¡€æ¨¡å‹ tokenizer è·¯å¾„ã€‚")
    parser.add_argument("--model-max-len", type=int, default=4096, help="æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ã€‚")
    parser.add_argument("--safety-buffer", type=int, default=20, help="ä¸ºé˜²æ­¢è¶…é•¿è€Œä¿ç•™çš„å®‰å…¨ token ä½™é‡ã€‚")
    # --- ç°æœ‰å‚æ•° ---
    parser.add_argument("--model-name", required=True, help="è¦è¯„ä¼°çš„ LoRA é€‚é…å™¨åç§°ã€‚")
    parser.add_argument("--input-file", required=True, help="åŒ…å«è¯„æµ‹æ•°æ®çš„ JSONL æ–‡ä»¶è·¯å¾„ã€‚")
    parser.add_argument("--output-file", required=True, help="ä¿å­˜è¯„æµ‹ç»“æœçš„ JSON æ–‡ä»¶è·¯å¾„ã€‚")
    parser.add_argument("--api-url", required=True, help="vLLM OpenAI API çš„ URLã€‚")
    parser.add_argument("--api-key", default="EMPTY", help="API å¯†é’¥ã€‚")
    parser.add_argument("--temperature", type=float, default=0.7, help="ç”Ÿæˆæ¸©åº¦ã€‚")
    args = parser.parse_args()

    # 1. åŠ è½½ Tokenizer å’Œè¾“å…¥æ•°æ®
    print("ğŸš€ Loading tokenizer for length calculation...")
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    
    prompts_data = []
    with open(args.input_file, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                prompts_data.append(json.loads(line))
    print(f"ğŸ“š Loaded {len(prompts_data)} prompts from {args.input_file}")

    # 2. é¢„å¤„ç†ï¼šè¿‡æ»¤è¶…é•¿ prompt å¹¶è®¡ç®—åŠ¨æ€ max_tokens
    tasks_to_run = []
    skipped_prompts = []
    print("ğŸ”§ Filtering long prompts and calculating dynamic max_tokens...")
    for original_data in prompts_data:
        # å¿…é¡»åœ¨å®¢æˆ·ç«¯æ¨¡æ‹ŸèŠå¤©æ¨¡æ¿ï¼Œä»¥è·å¾—æ­£ç¡®çš„è¾“å…¥é•¿åº¦
        chat_message = [{"role": "user", "content": original_data['prompt']}]
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸éœ€è¦å®Œæ•´çš„æ¨¡æ¿å­—ç¬¦ä¸²ï¼Œåªéœ€è¦ messages éƒ¨åˆ†çš„ token é•¿åº¦å³å¯
        # vLLM çš„ chat API ä¼šå¤„ç†æ¨¡æ¿ï¼Œä½†æˆ‘ä»¬éœ€è¦çŸ¥é“å®ƒå¤„ç†åçš„é•¿åº¦
        # tokenizer.apply_chat_template é»˜è®¤ä¼šæ·»åŠ æ¨¡æ¿ï¼Œæˆ‘ä»¬éœ€è¦çš„æ˜¯ prompt éƒ¨åˆ†çš„ token
        # æ›´å¥½çš„æ–¹å¼æ˜¯ç›´æ¥å¯¹ message å†…å®¹åˆ†è¯ï¼Œå¹¶åŠ ä¸Šæ¨¡æ¿è‡ªèº«çš„ token æ•°é‡ï¼ˆå¤§çº¦10-15ä¸ªï¼‰
        
        # ä¸€ä¸ªæ›´å‡†ç¡®çš„æ–¹æ³•ï¼šç›´æ¥å¯¹å®Œæ•´æ¨¡æ¿åˆ†è¯
        full_prompt_str = tokenizer.apply_chat_template(chat_message, tokenize=False, add_generation_prompt=True)
        input_ids = tokenizer.encode(full_prompt_str)
        num_input_tokens = len(input_ids)

        if num_input_tokens >= args.model_max_len - args.safety_buffer:
            skipped_prompts.append({
                **original_data, "reason": "Input prompt is too long",
                "num_input_tokens": num_input_tokens, "model_max_len": args.model_max_len
            })
            continue

        max_new_tokens = args.model_max_len - num_input_tokens - args.safety_buffer
        tasks_to_run.append({"prompt_data": original_data, "max_tokens": max_new_tokens})

    if skipped_prompts:
        print(f"âš ï¸ Skipped {len(skipped_prompts)} prompts because they were too long.")
        # å°†è¢«è·³è¿‡çš„ prompt ä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶ä¸­
        output_dir = os.path.dirname(args.output_file)
        skipped_file_path = os.path.join(output_dir, f"skipped_prompts_{args.model_name}.jsonl")
        with open(skipped_file_path, 'w', encoding='utf-8') as f:
            for item in skipped_prompts:
                f.write(json.dumps(item) + '\n')
        print(f"   (Details saved to {skipped_file_path})")

    if not tasks_to_run:
        print("âŒ No valid prompts left to generate. Creating an empty result file.")
        with open(args.output_file, "w") as f: json.dump([], f)
        return

    # 3. å¹¶å‘å¤„ç†æœ‰æ•ˆçš„è¯·æ±‚
    timeout = aiohttp.ClientTimeout(total=600) # 10åˆ†é’Ÿè¶…æ—¶
    async with aiohttp.ClientSession(timeout=timeout) as session:
        async_tasks = []
        for task_info in tasks_to_run:
            task = generate_response(
                session, args.api_url, args.api_key, args.model_name,
                task_info['prompt_data'], args.temperature, task_info['max_tokens']
            )
            async_tasks.append(task)
        
        all_results = await tqdm_asyncio.gather(
            *async_tasks, desc=f"Generating for {args.model_name}", total=len(async_tasks)
        )

    # 4. ä¿å­˜ç»“æœ
    with open(args.output_file, "w", encoding="utf-8") as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ è¯„æµ‹å®Œæˆã€‚ç»“æœå·²ä¿å­˜åˆ° {args.output_file}")


if __name__ == "__main__":
    asyncio.run(main())
