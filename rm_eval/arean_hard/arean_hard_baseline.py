import argparse
import json
import os
import torch
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
from tqdm import tqdm

def main():
    """
    ä½¿ç”¨ vLLM åº“ç›´æ¥åŠ è½½å¹¶è¯„ä¼°ä¸€ç³»åˆ—æœ¬åœ°æ¨¡å‹ã€‚
    """
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ vLLM åº“ç›´æ¥è¯„ä¼°åŸºçº¿æ¨¡å‹ã€‚")
    parser.add_argument("--baseline-models", nargs='+', default=[
        "ROOT/saved_llms/Llama-3-8b-sft-mixture",
        "ROOT/saved_llms/Meta-Llama-3.1-8B-Instruct"
    ], help="è¦è¯„ä¼°çš„åŸºçº¿æ¨¡å‹è·¯å¾„åˆ—è¡¨ã€‚")
    parser.add_argument("--input-file", default="ROOT/saved_data/arean-hard-v1.json", help="åŒ…å«è¯„æµ‹æ•°æ®çš„ JSONL æ–‡ä»¶è·¯å¾„ã€‚")
    parser.add_argument("--output-dir", default="evaluation_outputs", help="ä¿å­˜è¯„æµ‹ç»“æœçš„ç›®å½•ã€‚")
    parser.add_argument("--tensor-parallel-size", type=int, default=8, help="vLLM çš„å¼ é‡å¹¶è¡Œå¤§å°ã€‚")
    parser.add_argument("--temperature", type=float, default=0.7, help="ç”Ÿæˆæ¸©åº¦ã€‚")
    parser.add_argument("--max-tokens", type=int, default=4096, help="æœ€å¤§ç”Ÿæˆæ–° token æ•°ã€‚")
    args = parser.parse_args()

    # 1. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(args.output_dir, exist_ok=True)

    # 2. è¯»å–è¾“å…¥æ•°æ®
    prompts_data = []
    with open(args.input_file, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                prompts_data.append(json.loads(line))
    
    print(f"ğŸ“š æˆåŠŸåŠ è½½ {len(prompts_data)} æ¡ prompts ä» {args.input_file}")

    # 3. å¾ªç¯å¤„ç†æ¯ä¸ªåŸºçº¿æ¨¡å‹
    for model_path in args.baseline_models:
        model_name = os.path.basename(model_path)
        print(f"\n{'='*20} EVALUATING: {model_name} {'='*20}")
        
        # a. åŠ è½½æ¨¡å‹
        # è¿™æ˜¯æ•´ä¸ªæµç¨‹çš„æ ¸å¿ƒï¼Œç›´æ¥å®ä¾‹åŒ– LLM å¯¹è±¡
        print("ğŸš€ Loading model...")
        llm = LLM(
            model=model_path,
            tensor_parallel_size=args.tensor_parallel_size,
            trust_remote_code=True,
            # gpu_memory_utilization=0.9, # å¦‚æœéœ€è¦ï¼Œå¯ä»¥è°ƒæ•´æ˜¾å­˜ä½¿ç”¨ç‡
        )
        print("âœ… Model loaded.")

        # b. å‡†å¤‡å¸¦æ¨¡æ¿çš„ Prompts
        # éœ€è¦ä¸€ä¸ª tokenizer æ¥åº”ç”¨èŠå¤©æ¨¡æ¿
        # è¿™é‡Œçš„ tokenizer ä»…ç”¨äºæ¨¡æ¿æ ¼å¼åŒ–ï¼Œä¸ç”¨äºæ¨ç†
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        chat_messages = [[{"role": "user", "content": p['prompt']}] for p in prompts_data]
        formatted_prompts = [
            tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)
            for msg in chat_messages
        ]

        # c. è®¾ç½®é‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(
            temperature=args.temperature,
            max_tokens=args.max_tokens,
            stop=["<|eot_id|>", "<|end_of_text|>"]
        )

        # d. æ‰¹é‡ç”Ÿæˆå“åº” (vLLM ä¼šè‡ªåŠ¨å¤„ç†æ‰¹å¤„ç†å’Œè¿›åº¦æ¡)
        print("ğŸ’¬ Generating responses...")
        outputs = llm.generate(formatted_prompts, sampling_params)

        # e. æ ¼å¼åŒ–å¹¶ä¿å­˜ç»“æœ
        all_results = []
        # å°†è¾“å‡ºä¸åŸå§‹æ•°æ®æŒ‰é¡ºåºåŒ¹é…
        for i, output in enumerate(outputs):
            original_data = prompts_data[i]
            generated_text = output.outputs[0].text.strip()
            
            record = original_data.copy()
            record['response'] = generated_text
            record['generator'] = model_name
            all_results.append(record)
            
        # f. å†™å…¥æ–‡ä»¶
        output_file = os.path.join(args.output_dir, f"{model_name}.json")
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ’¾ è¯„æµ‹å®Œæˆã€‚ç»“æœå·²ä¿å­˜åˆ° {output_file}")
        
        # g. æ¸…ç†æ˜¾å­˜ï¼Œä¸ºä¸‹ä¸€ä¸ªæ¨¡å‹åšå‡†å¤‡ (è‡³å…³é‡è¦ï¼)
        del llm
        del tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print(f"ğŸ§¹ æ¨¡å‹ {model_name} å·²ä»å†…å­˜ä¸­å¸è½½ã€‚")

    print(f"\nğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰åŸºçº¿æ¨¡å‹è¯„ä¼°å®Œæˆï¼ ğŸ‰ğŸ‰ğŸ‰")

if __name__ == "__main__":
    main()
